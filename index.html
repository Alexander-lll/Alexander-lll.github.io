<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <meta name="description" content="Personal website of Alexander" />
  <meta name="keywords" content="Machine Learning Engineer, ML, Personal Website,
			personal website, HTML, CSS, SQL, Python, C++, Artifical Intelligence, EXCEL, Machine Learning" />
  <link rel="preconnect" href="https://fonts.gstatic.com" />
  <link href="https://fonts.googleapis.com/css2?family=Poppins:wght@400;500;600;700&display=swap" rel="stylesheet" />
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/devicons/devicon@v2.14.0/devicon.min.css" />
  <script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-5008414277567291"
    crossorigin="anonymous"></script>
  <script src="https://kit.fontawesome.com/fa62c117c7.js" crossorigin="anonymous"></script>
  <link rel="stylesheet" href="./css/index.css" />
  <link rel="icon" href="./public/meta-icon.avif" type="image/x-icon" />
  <title>Alexander</title>
</head>

<body class="body-light">
  <header id="top"> 
    <a class="icon-border header-title" href="#">A</a>
    <nav>
      <ul id="nav">
        <li>
          <a href="#research interests" class="link">Research Interests</a>
        </li>
        <li>
          <a href="#project" class="link">Projects</a>
        </li>
        <li>
          <a href="#stack" class="link">Tech Stack</a>
        </li>
        <li>
          <a href="#contact" class="link">Contact</a>
        </li>
	      <li>
        </li>
      </ul>
      <button aria-label="Toggle theme" id="toggleTheme">
        <i aria-hidden="true" class="icon-normal fas fa-moon"></i>
      </button>
      <button aria-label="Toggle navbar">
        <i aria-hidden="true" class="icon-normal fas fa-bars"></i>
      </button>
    </nav>
  </header>

  <div class="hero">
    <h1>
      Hi, I am
      <span class="hero-name">Alexander</span>
    </h1>

    <div class="hero-about">
      <p>
       Independent Machine Learning Engineer
	<p>
		<p>
			From SF Bay Area
		</p>
        <div><b> Status quo:</b> Multimodal AI Research</div>
      </p>

      <a class="researching" href="">
        
        <span class="btn">Researching</span>
      </a>
    </div>
  </div>

<!--   <section id="experience" class="experience">
    <h2 class="section-title">Experience</h2>
  
    <!-- Experience 1 (Ongoing) -->
    <div class="experience-card">
      <div class="experience-item">
        <div class="experience-logo">
          <img src="public\Overview of Brain2Music pipeline.png" alt="Brain2Music Pipeline" />
        </div>
        <div class="experience-details">
          <h3>Brain2Music: Reconstructing Music From Human Brain Activity</h3>
          <h4>Takuya Matsuyama, Andrea Agostinelli, Tomoya Nakai, Christian Frank, Shinji Nishimoto</h4>
          <p>July 20 2023 ·</p>
          <p>A method for reconstructing music from brain activity, captured using functional magnetic resonance imaging (fMRI) uses either music retrieval or the MusicLM music generation model conditioned on embeddings derived from fMRI data. The generated music resembles the musical stimuli that human subjects experienced, with respect to semantic properties like genre, instrumentation, and mood. We investigate the relationship between different components of MusicLM and brain activity through a voxel-wise encoding modeling analysis. Furthermore, we discuss which brain regions represent information derived from purely textual descriptions of music stimuli.</p>
        </div>
      </div>
    </div>
  
    <!-- Experience 2 (Ongoing) -->
    <div class="experience-card">
      <div class="experience-item">
        <div class="experience-logo">
          <img src="public\Method Diagram.png" alt="Speech From Brain Recording Model" />
        </div>
        <div class="experience-details">
          <h3>Decoding speech perception from non-invasive brain recordings</h3>
          <h4>Alexandre Défossez, Charlotte Caucheteux, Jérémy Rapin, Ori Kabeli, and Jean-Rémi King</h4>
          <p>August 25, 2022 ·</p>
          <p>Decoding language from brain activity is a long-awaited goal in both healthcare and neuroscience. Major milestones have recently been reached thanks to intracranial devices: subject-specific pipelines trained on invasive brain responses to basic language tasks now start to efficiently decode interpretable features (e.g. letters, words, spectrograms). However, scaling this approach to natural speech and non-invasive brain recordings remains a major challenge. Here, we propose a single end-to-end architecture trained with contrastive learning across a large cohort of individuals to predict self-supervised representations of natural speech. We evaluate our model on four public datasets, encompassing 169 volunteers recorded with magneto- or electro-encephalography (M/EEG), while they listened to natural speech. The results show that our model can identify, from 3s of MEG signals, the corresponding speech segment with up to 72.5% top-10 accuracy out of 1,594 distinct segments (and 44% top-1 accuracy), and up to 19.1% out of 2,604 segments for EEG recordings -- hence allowing the decoding of phrases absent from the training set. Model comparison and ablation analyses show that these performances directly benefit from our original design choices, namely the use of (i) a contrastive objective, (ii) pretrained representations of speech and (iii) a common convolutional architecture simultaneously trained across several participants. Together, these results delineate a promising path to decode natural language processing in real time from non-invasive recordings of brain activity.</p>
        </div>
      </div>
    </div>
  
    <!-- Experience 3  -->
    <div class="experience-card">
      <div class="experience-item">
        <div class="experience-logo">
          <img src="public\ Overview of NEUROIMAGEN.png" alt="NeuroImageGen Overview" />
        </div>
        <div class="experience-details">
          <h3>Seeing through the Brain: Image Reconstruction of Visual Perception from Human Brain Signals</h3>
          <h4>Yu-Ting Lan, Kan Ren, Yansen Wang, Wei-Long Zheng, Dongsheng Li, Bao-Liang Lu, Lili Qiu</h4>
          <p>July 27, 2023 · </p>
          <p>Seeing is believing, however, the underlying mechanism of how human visual perceptions are intertwined with our cognitions is still a mystery. Thanks to the recent advances in both neuroscience and artificial intelligence, we have been able to record the visually evoked brain activities and mimic the visual perception ability through computational approaches. In this paper, we pay attention to visual stimuli reconstruction by reconstructing the observed images based on portably accessible brain signals, i.e., electroencephalography (EEG) data. Since EEG signals are dynamic in the time-series format and are notorious to be noisy, processing and extracting useful information requires more dedicated efforts; In this paper, we propose a comprehensive pipeline, named NeuroImagen, for reconstructing visual stimuli images from EEG signals. Specifically, we incorporate a novel multi-level perceptual information decoding to draw multi-grained outputs from the given EEG data. A latent diffusion model will then leverage the extracted information to reconstruct the high-resolution visual stimuli images. The experimental results have illustrated the effectiveness of image reconstruction and superior quantitative performance of our proposed method.</p>
        </div>
      </div>
    </div>
  
    <!-- Experience  -->
    <div class="experience-card">
      <div class="experience-item">
        <div class="experience-logo">
          <img src="public\Brain decoding & video reconstruction.png" alt="Brain Decoding Video Reconstruction Model" />
        </div>
        <div class="experience-details">
          <h3>Cinematic Mindscapes: High-quality Video Reconstruction from Brain Activity</h3>
          <h4>Zijiao Chen, Jiaxin Qing, Juan Helen Zhou</h4>
          <p>May 19, 2023 ·</p>
          <p>Reconstructing human vision from brain activities has been an appealing task that helps to understand our cognitive process. Even though recent research has seen great success in reconstructing static images from non-invasive brain recordings, work on recovering continuous visual experiences in the form of videos is limited. In this work, we propose MinD-Video that learns spatiotemporal information from continuous fMRI data of the cerebral cortex progressively through masked brain modeling, multimodal contrastive learning with spatiotemporal attention, and co-training with an augmented Stable Diffusion model that incorporates network temporal inflation. We show that high-quality videos of arbitrary frame rates can be reconstructed with MinD-Video using adversarial guidance. The recovered videos were evaluated with various semantic and pixel-level metrics. We achieved an average accuracy of 85% in semantic classification tasks and 0.19 in structural similarity index (SSIM), outperforming the previous state-of-the-art by 45%. We also show that our model is biologically plausible and interpretable, reflecting established physiological processes.</p>
          <!-- <p>
            
          </p> -->
        </div>
      </div>
    </div>
  
    <!-- Experience 5 -->
    <div class="experience-card">
      <div class="experience-item">
        <div class="experience-logo">
          <img src="public\Overview of Recon3DMind task.png" alt="Overview of Recon3Dmind" />
        </div>
        <div class="experience-details">
          <h3>MinD-3D: Reconstruct High-quality 3D objects in Human Brain</h3>
          <h4>Jianxiong Gao, Yuqian Fu, Yun Wang, Xuelin Qian, Jianfeng Feng, Yanwei Fu</h4>
          <p>December 12, 2023 ·</p>
          <p>Abstract. In this paper, we introduce Recon3DMind, an innovative task aimed at reconstructing 3D visuals from Functional Magnetic Resonance Imaging (fMRI) signals, marking a significant advancement in the fields of cognitive neuroscience and computer vision. To support this pioneering task, we present the fMRI-Shape dataset, which includes data from 14 participants and features 360-degree videos of 3D objects to enable comprehensive fMRI signal capture across various settings, thereby laying a foundation for future research. Furthermore, we propose MinD-3D, a novel and effective three-stage framework specifically designed to decode the brain’s 3D visual information from fMRI signals, demonstrating the feasibility of this challenging task. The framework begins by extracting and aggregating features from fMRI frames through a neuro-fusion encoder, subsequently employs a feature bridge diffusion model to generate visual features, and ultimately recovers the 3D object via a generative transformer decoder. We assess the performance of MinD-3D using a suite of semantic and structural metrics and analyze the correlation between the features extracted by our model and the visual regions of interest (ROIs) in fMRI signals. Our findings indicate that MinD-3D not only reconstructs 3D objects with high semantic relevance and spatial similarity but also significantly enhances our understanding of the human brain’s capabilities in processing 3D visual information. Project page at: https://jianxgao.github.io/MinD-3D.</p>
          <!-- <p>
            
          </p> -->
        </div>
      </div>
    </div>
  </section> -->
  


  <section id="project" class="project">
    <h2 class="section-title">Projects I Use</h2>
    <!-- 0th  -->
    <div class="project-container">
      <img src="/public/transformers.png" alt="Transformers" height="250"
        width="450" />

      <div class="project-content">
        <h3>Transormers: State-of-the-art Machine Learning for Pytrorch, TensorFlow, and JAX</h3>
        <p>
          Transformers is a library of pretrained text, computer vision, audio, video, and multimodal models for inference and training. Use Transformers to fine-tune models on your data, build inference applications, and for generative AI use cases across multiple modalities.
        </p>

        <ul class="project-skill">
          <li>Python</li>
          <li>NLP</li>
          <li>Machine Learning</li>
          <li>Huggingface</li>
          <li>Language Models</li>
        </ul>

        <ul class="project-link">
          <li>
            <a href="https://github.com/huggingface/transformers" aria-label="Source code"
              title="source code" rel="noopener"><i class="icon-border fas fa-code"></i></a>
          </li>
          <li>
            <a href="https://huggingface.co/transformers" aria-label="Preview" title="preview"
              rel="noopener"><i aria-hidden="true" class="icon-border fas fa-external-link-alt"></i></a>
          </li>
        </ul>
      </div>
    </div>
    
    <!-- 1st  -->
    <div class="project-container">
      <img src="/public/diffusers.png" alt="Diffusers" height="250" width="450" />

      <div class="project-content">
        <h3>Diffusers: State-of-the-art diffusion models for image, video and audio generation in PyTorch and FLAX</h3>
        <p>
           Diffusers is the go-to library for state-of-the-art pretrained diffusion models for generating images, audio, and even 3D structures of molecules. Whether you're looking for a simple inference solution or training your own diffusion models, Diffusers is a modular toolbox that supports both. The library is designed with a focus on usability over performance, simple over easy, and customizability over abstractions.
        </p>

        <ul class="project-skill">
          <li>Deep-Learning</li>
          <li>Pytorch</li>
          <li>JAX</li>
          <li>Image-Generation</li>
          <li>Diffusion</li>
          <!-- <li>JavaScript</li>
					<li>React.js</li> -->
        </ul>

        <ul class="project-link">
          <li>
            <a href="https://github.com/huggingface/diffusers" aria-label="Source code"
              title="source code" rel="noopener"><i class="icon-border fas fa-code"></i></a>
          </li>
          <li>
            <a href="https://huggingface.co/docs/diffusers" aria-label="Preview" title="preview"
              rel="noopener"><i aria-hidden="true" class="icon-border fas fa-external-link-alt"></i></a>
          </li>
        </ul>
      </div>
    </div>

    <!-- 2nd  -->
    <div class="project-container">
      <img src="/public/gradio.png" alt="Gradio" height="250" width="450" />

      <div class="project-content">
        <h3>Gradio: Build and share delightful machine learning apps, all in Python.</h3>
        <p>
          Gradio is an open-source Python package that allows you to quickly build a demo or web application for your machine learning model, API, or any arbitrary Python function. You can then share a link to your demo or web application in just a few seconds using Gradio's built-in sharing features. No JavaScript, CSS, or web hosting experience needed
        </p>

        <ul class="project-skill">
          <li>Python</li>
          <li>Data-Science</li>
          <li>UI-Components</li>
          <li>Data-Analysis</li>

          <!-- <li>JavaScript</li>
					<li>React.js</li> -->
        </ul>

        <ul class="project-link">
          <li>
            <a href="https://github.com/gradio-app/gradio" aria-label="Source code"
              title="source code" rel="noopener"><i class="icon-border fas fa-code"></i></a>
          </li>
          <li>
            <a href="http://www.gradio.app/" aria-label="Preview" title="preview"
              rel="noopener"><i aria-hidden="true" class="icon-border fas fa-external-link-alt"></i></a>
          </li>
        </ul>
      </div>
    </div>

    <!-- 3rd  -->
    <div class="project-container">
      <img src="/public/230134379-7181e485-c521-4d23-a0d6-f7b3b61ba524.png" alt="LLaMA C++" height="250" width="450" />

      <div class="project-content">
	<h3>LLaMA C++: LLM Inference in C/C++</h3>
        <p>
          LLaMA C++ enables LLM inference with minimal setup and state-of-the-art performance on a wide range of hardware - locally and in the cloud
        </p>

        <ul class="project-skill">
          <li>Python</li>
          <li>GGUF</li>
          <li>GMML</li>
          <li>CUDA</li>

          <!-- <li>JavaScript</li>
					<li>React.js</li> -->
        </ul>

        <ul class="project-link">
          <li>
            <a href="https://github.com/ggml-org/llama.cpp" aria-label="Source code"
              title="source code" rel="noopener"><i class="icon-border fas fa-code"></i></a>
          </li>
          <li>
            <a href="https://github.com/ggml-org/llama.cpp" aria-label="Preview" title="preview"
              rel="noopener"><i aria-hidden="true" class="icon-border fas fa-external-link-alt"></i></a>
          </li>
        </ul>
      </div>
    </div>

    <!-- 4th  -->
    <div class="project-container">
      <img src="/public/vllm-logo-text-dark.png" alt="vLLM" height="250"
        width="450" />

      <div class="project-content">
        <h3>vLLM: Fast and easy-to-use library for LLM Inference and Serving</h3>
        <p>
          A high-throughput and memory-efficient inference and serving engine for LLMs
        </p>

        <ul class="project-skill">
          <li>Python</li>
          <li>Inference</li>
          <li>CUDA</li>
          <li>AMD</li>
          <li>Model-Serving</li>
        </ul>

        <ul class="project-link">
          <li>
            <a href="https://github.com/vllm-project/vllm" aria-label="Source code"
              title="source code" rel="noopener"><i class="icon-border fas fa-code"></i></a>
          </li>
          <li>
            <a href="https://docs.vllm.ai/" aria-label="Preview" title="preview"
              rel="noopener"><i aria-hidden="true" class="icon-border fas fa-external-link-alt"></i></a>
          </li>
        </ul>
      </div>
    </div>
  </section>
.
  <section id="stack" class="stack">
    <h2 class="section-title">Tech Stack</h2>
    <div class="stack-container">
      <div class="tech-container">
        <i class="devicon-cplusplus-plain colored"></i>
        C++
      </div>
      <div class="tech-container">
        <i class="devicon-rust-plain colored"></i>
        Rust
      </div>
      <div class="tech-container">
        <i class="devicon-linux-plain colored"></i>

        Linux
      </div>
      <div class="tech-container">
        <i class="devicon-python-plain colored"></i>

        Python
      </div>
      <div class="tech-container">

	      
        <img src="https://cdn.jsdelivr.net/gh/devicons/devicon/icons/pytorch/pytorch-original.svg" width="50" />
	      
        Pytorch
      </div>
      <!-- <div class="tech-container">
				<i class="devicon-dart-plain colored"></i>
				Dart
			</div>
			<div class="tech-container">
				<i class="devicon-flutter-plain colored"></i>
				Flutter
			</div> -->
      <div class="tech-container">
        <i class="devicon-tensorflow-original colored"></i>
	      
        Tensorflow
      </div>
      <div class="tech-container">

	      
        <img src="https://cdn.jsdelivr.net/gh/devicons/devicon/icons/fastapi/fastapi-original.svg" width="50" />
	      
	      FastAPI
      </div>

      <div class="tech-container">
        <i class="devicon-mysql-plain colored"></i>

        Mysql
      </div>
      <div class="tech-container">
        <i class="devicon-mongodb-plain colored"></i>

        MongoDB
      </div>
      <div class="tech-container">
        <i class="devicon-html5-plain colored"></i>

        HTML5
      </div>
      <div class="tech-container">
        <i class="devicon-css3-plain"></i>

        CSS3
      </div>
      <div class="tech-container">
        <i class="devicon-docker-plain"></i>

        Docker
      </div>
      <div class="tech-container">
        <i class="devicon-kubernetes-plain colored"></i>

        Kubernetes
      </div>
      <!-- <div class="tech-container">
				<i class="devicon-linux-plain"></i>
				Linux
			</div> -->
    </div>
  </section>

  <section id="contact" class="contact">
    <h2 class="section-title">Get in touch</h2>

    <ul>
      <li>
        <a href="mailto:superuser01001@icloud.com">
          <span class="btn">
            <i aria-hidden="true" class="fas icon fa-envelope"></i>Email
          </span>
        </a>
      </li>
    </ul>
  </section>

  <footer>
    <small> Alexander </small>
  </footer>

  <div class="scroll-container">
    <div aria-label="Scroll up" class="scroll-up">
      <a href="#top">
        <i aria-hidden="true" class="fas fa-arrow-up"></i>
      </a>
    </div>
  </div>
  <script src="./js/index.js"></script>
</body>
  <!-- 
				Credits to AK 
		 -->
</html>
